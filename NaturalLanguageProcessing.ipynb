{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaturalLanguageProcessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    },
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttya16/NaturalLanguageProcessing-zeroDL2/blob/master/NaturalLanguageProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUcNgG98Pp5w",
        "colab_type": "text"
      },
      "source": [
        "# 自然言語処理(Natural Language Processing)\n",
        "自然言語を処理する分野＝私たちが普段使っている言葉をコンピューターに理解させるための技術\n",
        "\n",
        "### コンピューターが理解する言語\n",
        "→プログラミング言語、マークアップ言語<br>\n",
        "・・・これらは記述が一意に解釈できるよう文法が定義されているため、機械的で無機質なものと言える。\n",
        "\n",
        "\n",
        "\n",
        "### 自然言語\n",
        "→日本語、英語など普段私たち人間が使っている言語\n",
        "\n",
        "・・・これらは同じ記述でも異なる意味をもつ可能性があったり、同じ意味でも表現方法が何通りもあったりと、柔軟性を併せ持ったものと言える。\n",
        "時代と共に新たな言葉が加わったり、従来の意味が変化したりという性質もある。\n",
        "\n",
        "\n",
        "コンピューターが自然言語を理解できるようになれば、もっと人にとって役立つことを行わせることができるようになる。<br>\n",
        "例.機械翻訳、文章の自動要約、感情分析\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFGVN1qVPp5x",
        "colab_type": "text"
      },
      "source": [
        "## 単語の意味\n",
        "コンピューターに単語の意味を理解させるにはどうすれば良いか。<br>\n",
        "↓<br>\n",
        "従来の３つの手法\n",
        "* シソーラス \n",
        "* カウントベース\n",
        "* 推論ベース(word2vec)\n",
        "\n",
        "\n",
        "### シソーラス\n",
        "類語辞書のようなもの。同じ意味or似た意味の単語は同じグループに分類されている。<br>\n",
        "(自然言語処理において利用されるシソーラスでは単語間に上下関係や包含関係といった細かい関係性が設定されているケースもある)\n",
        "\n",
        "ex. car = auto automobile machine motorcar \n",
        "\n",
        "シソーラスの代表的なもの　＝　WordNet　　\n",
        "NLTKというモジュールをインストールして使うことができる。\n",
        "\n",
        "#### 欠点\n",
        "* 時代の変化に伴う、新単語の追加や単語の意味の変遷に対応するのが困難\n",
        "* シソーラスを作成するには手間が膨大にかかる\n",
        "* 単語の意味は捉えられても、それぞれの単語が持つニュアンスまでは捉えられない"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zQUAPbEPp5y",
        "colab_type": "text"
      },
      "source": [
        "### カウントベースの手法\n",
        "コーパスの準備・・・カウントベース手法による単語の意味表現を行うために、まずは解析のために収集されたテキストデータ（=コーパス）を用意する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P2RfltPPp5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_text = \"You say goodbye and I say hello.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxq0rGrQPp52",
        "colab_type": "code",
        "colab": {},
        "outputId": "3a45c61d-3479-42c5-a589-0c5f15993a4f"
      },
      "source": [
        "text = sample_text.lower() #全部小文字で統一する\n",
        "text = text.replace(\".\", \" .\") #最後のピリオドは単体で分けて捉える。\n",
        "\n",
        "words = text.split(' ') #スペース区切りで単語リストを取得\n",
        "words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDGg3rNjPp58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#単語にIDを振って、処理しやすい形にする\n",
        "#word_to_id : 単語からIDを参照するdict\n",
        "#id_to_word：IDから単語を参照するdict\n",
        "#corpus：単語IDのリスト\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace(\".\", \" .\")\n",
        "    words = text.split(' ')\n",
        "    \n",
        "    w_to_id = {}\n",
        "    id_to_w = {}\n",
        "    new_id = 0\n",
        "    for w in words:\n",
        "        if w not in w_to_id:\n",
        "            w_to_id[w] = new_id\n",
        "            id_to_w[new_id] = w\n",
        "            new_id += 1\n",
        "            \n",
        "    corpus = np.array([w_to_id[word] for word in words])\n",
        "        \n",
        "    return corpus, w_to_id, id_to_w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adn6Ux_DPp5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus, word_to_id, id_to_word = preprocess(sample_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6qX13zBPp6B",
        "colab_type": "code",
        "colab": {},
        "outputId": "ce8973d2-932c-4097-eb51-d397e71e0fa4"
      },
      "source": [
        "print(word_to_id)\n",
        "print(id_to_word)\n",
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'hello': 5, 'say': 1, '.': 6, 'you': 0, 'and': 3, 'goodbye': 2, 'i': 4}\n",
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
            "[0 1 2 3 4 1 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFCPvl5VPp6H",
        "colab_type": "text"
      },
      "source": [
        "### 単語の分散表現\n",
        "簡単に言えば、単語の意味をベクトルで表すこと。<br>例えば色はRGBの各成分がどれくらい混ぜ合わさっているかを(R, G, B) = (201, 23, 30)のように表現することができる。<br>これと似たようなことを単語に対しても行えないか、という発想。\n",
        "\n",
        "#### 分布仮説\n",
        "「単語の意味は周囲の単語によって形成される」というもので、単語をベクトル表現するという手法の根底にある仮説である。<br>\n",
        "テキスト中の注目したい単語の周囲にある単語のことを**コンテキスト**と呼び、コンテキストのサイズとは前後の何単語くらいを含めるかを表す。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYjzHaGIPp6I",
        "colab_type": "text"
      },
      "source": [
        "#### 共起行列\n",
        "テキスト中の各単語のコンテキストをカウントして、行列で表現したもの。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HOhOZTSPp6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#テキストから共起行列を作成する関数\n",
        "\n",
        "def co_occurence(text, window_size=1):\n",
        "    corpus, word_to_id, id_to_word =preprocess(text)\n",
        "    mat_size = len(word_to_id)\n",
        "    C = np.zeros((mat_size, mat_size))\n",
        "    \n",
        "    for i in range(len(corpus)):\n",
        "        for j in range(window_size):\n",
        "            if i - j - 1  >= 0:\n",
        "                C[corpus[i] , corpus[i - j -1 ]] += 1\n",
        "            if i + j + 1 <= len(corpus) - 1:\n",
        "                C[corpus[i], corpus[i + j + 1]] += 1\n",
        "    \n",
        "    C = np.array(C, dtype=np.int32)\n",
        "    return C"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plzNzi9gPp6L",
        "colab_type": "code",
        "colab": {},
        "outputId": "b47213c2-9f64-4007-a464-15b223f590a3"
      },
      "source": [
        "co_occurence(sample_text, window_size=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 0, 0, 0, 0],\n",
              "       [1, 0, 1, 2, 1, 1, 1],\n",
              "       [1, 1, 0, 1, 1, 0, 0],\n",
              "       [0, 2, 1, 0, 1, 0, 0],\n",
              "       [0, 1, 1, 1, 0, 1, 0],\n",
              "       [0, 1, 0, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 0, 0, 1, 0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8-Q0zdxPp6O",
        "colab_type": "text"
      },
      "source": [
        "### ベクトル間の類似度\n",
        "テキスト中の単語のベクトル表現が共起行列によって可能になったら、次は単語間の類似度を測る指標が必要になる。\n",
        "単純にユークリッド距離や内積を求めるのも良いが、単語の類似度は**コサイン類似度**が主に用いられる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y6tzvSGPp6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cosine similarity\n",
        "#eps:ゼロ除算を防ぐための微小な値\n",
        "def cos_similarity(x, y, eps=1e-8):\n",
        "    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n",
        "    ny = y / (np.sqrt(np.sum(y**2)) + eps)\n",
        "    \n",
        "    return np.dot(nx, ny)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iCCSORgPp6S",
        "colab_type": "code",
        "colab": {},
        "outputId": "a7948f26-b377-46b8-a2f4-57805865e4a2"
      },
      "source": [
        "C = co_occurence(sample_text)\n",
        "\n",
        "c0 = C[word_to_id['you']]\n",
        "c1 = C[word_to_id['i']]\n",
        "\n",
        "print(cos_similarity(c0, c1)) #テキスト中の\"you\"という単語と\"i\"という単語の類似度がでる"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7071067691154799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYyqNb-aPp6V",
        "colab_type": "text"
      },
      "source": [
        "## 類似単語のランキング表示\n",
        "類似度計算の関数を使って、単語の類似度ランキングを返す関数を用意してみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKbgeC7mPp6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#query: 単語のクエリ\n",
        "#word_to_id : 単語からIDを参照するdict\n",
        "#id_to_word：　IDから単語を参照するdict\n",
        "#word_matrix:　単語ベクトルをまとめた行列。各行に対応する単語のベクトルが格納されている。\n",
        "\n",
        "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
        "    #take out the query\n",
        "    if query not in word_to_id:\n",
        "        print(\"%s is not found in database.\" %query)\n",
        "        return\n",
        "    \n",
        "    print('\\n[query]' + query)\n",
        "    query_id = word_to_id[query]\n",
        "    query_vec = word_matrix[query_id]\n",
        "    \n",
        "    #cosine similarity\n",
        "    vocab_size = len(id_to_word)\n",
        "    similarity = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "        similarity[i] = cos_similarity(query_vec, word_matrix[i])\n",
        "        \n",
        "    #sort as the highest similarity values\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if id_to_word[i] == query:\n",
        "            continue\n",
        "            \n",
        "        print('%s : %s' % (id_to_word[i], similarity[i]))\n",
        "        \n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxBg8bySPp6Y",
        "colab_type": "code",
        "colab": {},
        "outputId": "9719139d-ad47-4df7-a642-7d4f92be1752"
      },
      "source": [
        "text = \"A true leader has a confidence to stand alone, a courage to make tough decisions and a compassion to listen to the needs of others.\"\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "C = co_occurence(text)\n",
        "vocab_size = len(word_to_id)\n",
        "\n",
        "most_similar('stand', word_to_id, id_to_word, C, top=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[query]stand\n",
            "listen : 0.7071067726510136\n",
            "make : 0.49999999292893216\n",
            "the : 0.49999999292893216\n",
            "compassion : 0.49999999292893216\n",
            "confidence : 0.49999999292893216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reoBAEgtPp6b",
        "colab_type": "text"
      },
      "source": [
        "## 相互情報量\n",
        "共起行列では２つの単語が共起した回数に注目しているが、それだと不十分。<br>なぜなら、\"the\"や\"a\"といった高頻出単語の影響で単語同士の関連性が見えづらくなることがある。<br>\n",
        "例. driveとcarは関連性が強いはずだが、それよりも\"the car\"という並びでの出現回数が多いため、こちらの方が関連性では強いことになってしまう。\n",
        "\n",
        "\n",
        "そこで新たに**相互情報量(Pointwise Mutual Information)**という指標を考える。\n",
        "\n",
        "$$\n",
        "PMI(x, y) = \\log{_2}\\frac{P(x, y)}{P(x)P(y)}\n",
        "$$\n",
        "\n",
        "P(x, y) :　x, yが同時に起きる確率。ここでは共起する確率を表す。\n",
        "\n",
        "相互情報量によって２単語の共起回数に加え、各単語の出現回数を考慮することができるようになるため、高頻出単語に対しても正確な指標となって現れる。\n",
        "\n",
        "※共起回数が0の場合、PMIは-∞の値を取ってしまうため実装の際は正の相互情報量を用いるなどの工夫が必要。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VODJf5jxPp6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Positive PMI\n",
        "def ppmi(C, verbose=False, eps=1e-8):\n",
        "    M = np.zeros_like(C, dtype=np.float32)\n",
        "    N = np.sum(C)\n",
        "    S = np.sum(C, axis=0)\n",
        "    total = C.shape[0] * C.shape[1]\n",
        "    cnt = 0\n",
        "    \n",
        "    for i in range(C.shape[0]):\n",
        "        for j in range(C.shape[1]):\n",
        "            pmi = np.log2(C[i, j] * N / (S[j] * S[i]) + eps)\n",
        "            M[i, j] = max(0, pmi)\n",
        "            \n",
        "            if verbose:\n",
        "                cnt += 1\n",
        "                if cnt % (total//100) == 0:\n",
        "                    print('%.1f%% done' % (100*cnt/total))\n",
        "                    \n",
        "    return M\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac7SsgGoPp6e",
        "colab_type": "code",
        "colab": {},
        "outputId": "ab05e135-83a5-4dd1-d72c-84a7bcdfe6ed"
      },
      "source": [
        "text = \"You say goodbye and I say hello.\"\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "C = co_occurence(text, 3)\n",
        "vocab_size = len(word_to_id)\n",
        "print(np.sum(C, axis=0))\n",
        "print(np.sum(C))\n",
        "print(C)\n",
        "print(ppmi(C))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 9 5 6 6 4 3]\n",
            "36\n",
            "[[0 1 1 1 0 0 0]\n",
            " [1 0 2 2 2 1 1]\n",
            " [1 2 0 1 1 0 0]\n",
            " [1 2 1 0 1 1 0]\n",
            " [0 2 1 1 0 1 1]\n",
            " [0 1 0 1 1 0 1]\n",
            " [0 1 0 0 1 1 0]]\n",
            "[[0.0000000e+00 4.1503751e-01 1.2630345e+00 1.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00]\n",
            " [4.1503751e-01 0.0000000e+00 6.7807192e-01 4.1503751e-01 4.1503751e-01\n",
            "  1.4426950e-08 4.1503751e-01]\n",
            " [1.2630345e+00 6.7807192e-01 0.0000000e+00 2.6303440e-01 2.6303440e-01\n",
            "  0.0000000e+00 0.0000000e+00]\n",
            " [1.0000000e+00 4.1503751e-01 2.6303440e-01 0.0000000e+00 1.4426950e-08\n",
            "  5.8496249e-01 0.0000000e+00]\n",
            " [0.0000000e+00 4.1503751e-01 2.6303440e-01 1.4426950e-08 0.0000000e+00\n",
            "  5.8496249e-01 1.0000000e+00]\n",
            " [0.0000000e+00 1.4426950e-08 0.0000000e+00 5.8496249e-01 5.8496249e-01\n",
            "  0.0000000e+00 1.5849625e+00]\n",
            " [0.0000000e+00 4.1503751e-01 0.0000000e+00 0.0000000e+00 1.0000000e+00\n",
            "  1.5849625e+00 0.0000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt2lhI2zPp6q",
        "colab_type": "text"
      },
      "source": [
        "## 次元削減\n",
        "共起行列やPPMI行列は語彙数が増えるとそのサイズも大きくなり、かつその要素のほとんどが0である（疎行列）。<br>\n",
        "このようなベクトルはノイズの影響を受けやすく、頑健性に乏しい。<br>\n",
        "そこでベクトルの次元を削減することを考えるのだが、**重要な情報はできる限り残す**ことがポイント。<br>代表的な手法として特異値分解がある。\n",
        "<br>\n",
        "* 特異値分解(SVD)\n",
        "$$\n",
        "X = USV^T\n",
        "(U, V : 直交行列　　  S : 対角行列)\n",
        "$$\n",
        "\n",
        "np.linalg.svdでSVDの計算が可能。<br>\n",
        "しかしSVDの計算は$O(n^3)$になる為、大きいサイズの行列を扱う場合は注意。<br>\n",
        "例えばTruncated SVD(特異値の小さいものは省いて計算)を用いたりして工夫する必要がある。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFQ5P19MPp6r",
        "colab_type": "code",
        "colab": {},
        "outputId": "7ca081ae-d5df-4aa4-f543-93936e8c079a"
      },
      "source": [
        "#SVD\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = \"You say goodbye and I say hello.\"\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "C = co_occurence(text)\n",
        "W = ppmi(C)\n",
        "\n",
        "U, S, V = np.linalg.svd(W)\n",
        "print(U.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs2uVe9mPp62",
        "colab_type": "code",
        "colab": {},
        "outputId": "8b264a4e-1626-4587-94e3-3775a3250632"
      },
      "source": [
        "for word, word_id in word_to_id.items():\n",
        "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
        "    \n",
        "plt.scatter(U[:, 0], U[:, 1], alpha=0.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFxEJkMJfRCRACn8RkQAp/EVEAvT/ARusbaWJnJ3GAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8lzjqrRPp7B",
        "colab_type": "text"
      },
      "source": [
        "## PTBデータセットでの評価\n",
        "\n",
        "ここまではサンプルテキストから生成した非常に小さなコーパスを用いてきたが、より大きな本格的なコーパスを用いてみる。<br>\n",
        "Penn Treebankコーパスは適度な大きさのコーパスで、ベンチマーク用にも適している。\n",
        "\n",
        "[ゼロから始めるディープラーニング2のGitHubのptb.py](\"https://github.com/oreilly-japan/deep-learning-from-scratch-2/blob/master/dataset/ptb.py\")によってPTBデータセットをダウンロード。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZIyTAr3Pp7C",
        "colab_type": "code",
        "colab": {},
        "outputId": "e0d75327-10d2-4e82-defb-9d346858a7af"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mLICENSE.md\u001b[m\u001b[m                      \u001b[34mch05\u001b[m\u001b[m\r\n",
            "NaturalLanguageProcessing.ipynb \u001b[34mch06\u001b[m\u001b[m\r\n",
            "\u001b[31mREADME.md\u001b[m\u001b[m                       \u001b[34mch07\u001b[m\u001b[m\r\n",
            "\u001b[34mch01\u001b[m\u001b[m                            \u001b[34mch08\u001b[m\u001b[m\r\n",
            "\u001b[34mch02\u001b[m\u001b[m                            \u001b[34mcommon\u001b[m\u001b[m\r\n",
            "\u001b[34mch03\u001b[m\u001b[m                            \u001b[34mdataset\u001b[m\u001b[m\r\n",
            "\u001b[34mch04\u001b[m\u001b[m\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5Sn9WUjPp7H",
        "colab_type": "code",
        "colab": {},
        "outputId": "a66ae81f-03aa-4356-929b-727ae9dea35a"
      },
      "source": [
        "#Penn Treebankデータセット\n",
        "import ptb\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ptb.train.txt ... \n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdCNSC_tPp7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def co_occurence_fromcorpus(corpus, vocab_size, window_size=1):\n",
        "    C = np.zeros((vocab_size, vocab_size))\n",
        "    \n",
        "    for i in range(len(corpus)):\n",
        "        for j in range(window_size):\n",
        "            if i - j - 1  >= 0:\n",
        "                C[corpus[i] , corpus[i - j -1 ]] += 1\n",
        "            if i + j + 1 <= len(corpus) - 1:\n",
        "                C[corpus[i], corpus[i + j + 1]] += 1\n",
        "    \n",
        "    C = np.array(C, dtype=np.int32)\n",
        "    return C\n",
        "\n",
        "C = co_occurence_fromcorpus(corpus, len(word_to_id))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYu_13LxPp7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = ppmi(C)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po7kVD1LPp7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "U, S, V = randomized_svd(W, n_components=100, n_iter=5, random_state = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKuOjHZMPp7W",
        "colab_type": "text"
      },
      "source": [
        "## 推論ベースの手法\n",
        "\n",
        "単語をベクトルで表現する方法としてもう一つ挙げられるものとして、**推論ベースの手法**がある。<br>\n",
        "カウントベース手法とはアプローチの仕方が異なるが、いずれも分布仮説に基づいた手法となっている。\n",
        "\n",
        "### 推論ベース手法の概要\n",
        "以下のようなテキストの\"？\"の部分にどの単語が来るかという「推論」を行う。<br>\n",
        "\n",
        "**you ? goodbye and I say hello.**<br>\n",
        "前後にある単語から？にはどういう単語が当てはまるかを、単語の出現パターンを学習することで推測できるようなモデルを考える。<br>\n",
        "モデルとしてはニューラルネットを用いる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2ubiBvgPp7X",
        "colab_type": "text"
      },
      "source": [
        "#### カウントベース手法の問題点\n",
        "カウントベース手法では、単語の共起行列（→PPMI行列）を作り、SVDを用いて次元削減を行い、単語の分散表現を獲得するという流れだった。<br>\n",
        "ところが大規模なコーパス(語彙数100万など)を扱うとなると、100万×100万サイズの行列に対してSVDを実行しなければならない。<br>\n",
        "SVDは入力サイズn×n行列に対して計算量がO(n^3)とかかる為、n=100万の場合は現実的ではないことは明らかである。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYGKkhgKPp7Y",
        "colab_type": "text"
      },
      "source": [
        "## one-hot vector\n",
        "単語をそのままニューラルネットに処理させることはできないので、ここでもまたベクトル表現に変換してあげる必要がある。<br>\n",
        "ニューラルネットの場合は固定長のベクトルに変換する必要がある為、**one-hot表現**によるベクトル変換がよく用いられる。\n",
        "\n",
        "$$\n",
        "\\left[\\begin{array}{c}\n",
        "you(id=0)\\\\\n",
        "goodbye(id=2)\\\\\n",
        "\\end{array}\\right]  =\n",
        "\\left[\\begin{array}{c}\n",
        "1&0&0&0&0&0&0\\\\\n",
        "0&0&1&0&0&0&0\\\\\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "one-hot vector表現では単語IDの該当する箇所に1を、残りは0を設定する。<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc8nRZmwPp7Y",
        "colab_type": "code",
        "colab": {},
        "outputId": "e51546d2-1408-42eb-ce7e-b10d8794b9f0"
      },
      "source": [
        "\"\"\"one-hot vector表現の単語をNNの全結合層によって変換するイメージ例\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "c = np.array([[1, 0, 0, 0, 0, 0, 0]]) #input\n",
        "w = np.random.randn(7, 3) #weight\n",
        "h = np.dot(c, w) #hidden layer node\n",
        "print(h)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.0877564  -0.05846083 -0.12946881]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZbFVGmVPp7c",
        "colab_type": "text"
      },
      "source": [
        "上で行なっているのは、**ID=0の単語に対する重みの行ベクトルを抜き出す**という処理に他ならない。<br>\n",
        "わざわざ重みの行ベクトルを抜き出すためだけに行列計算をするのは効率が悪いと感じられる。→改良の余地あり"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3LLrqtLPp7e",
        "colab_type": "text"
      },
      "source": [
        "## シンプルなword2vec\n",
        "\n",
        "コンテキスト　→　モデル　→　確率分布<br>\n",
        "この流れで推論ベースの単語ベクトルを作成するが、このモデルをNNで設計していく。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9KUkBb8Pp7h",
        "colab_type": "text"
      },
      "source": [
        "### CBOW(contiuous bag-of-words)\n",
        "コンテキストからターゲット（コンテキストに対する中央の単語）を推測することを目的としたNN。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2_8p71ZPp7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NNの準備　MatMul layer\n",
        "\n",
        "class MatMul:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.x = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        W, = self.params\n",
        "        out = np.dot(x, W)\n",
        "        self.x = x\n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        W, = self.params\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dw = np.dot(self.x.T, dout)\n",
        "        self.grads[0][...] = dw\n",
        "        return dx\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmfh7jmDPp7k",
        "colab_type": "code",
        "colab": {},
        "outputId": "199d7faf-24b4-4340-d32c-782cef25377e"
      },
      "source": [
        "#CBOW\n",
        "\n",
        "#sample context data\n",
        "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
        "\n",
        "#initialize weight matrix\n",
        "W_in = np.random.randn(7, 3)\n",
        "W_out = np. random.randn(3, 7)\n",
        "\n",
        "#layers\n",
        "in_layer0 = MatMul(W_in)\n",
        "in_layer1 = MatMul(W_in)\n",
        "out_layer = MatMul(W_out)\n",
        "\n",
        "#feed forward\n",
        "h0 = in_layer0.forward(c0)\n",
        "h1 = in_layer1.forward(c1)\n",
        "h = 0.5*(h0 + h1)\n",
        "s = out_layer.forward(h)\n",
        "\n",
        "print(s)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.68594431 -0.40587038  0.92271091  0.43235151  0.3998735  -0.378036\n",
            "   1.05701179]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4ZFUYn5Pp7o",
        "colab_type": "text"
      },
      "source": [
        "上のシンプルCBOWの例では出力として各単語のスコアが算出されており、これに対してsoftmax関数を適用させてやれば確率を得ることができる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkH8NJCfPp7p",
        "colab_type": "text"
      },
      "source": [
        "### CBOWモデルの学習\n",
        "* word2vecモデルの学習の目標は、**コンテキスト（入力）を元に、コンテキストの中心に来るべきターゲット（出力）の出現確率を最大にする**ことである。\n",
        "* 学習されるのは重みW_in, W_outだが、これらがまさに単語の出現パターンを捉えたmatrixとなる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3TDi1P0Pp7q",
        "colab_type": "text"
      },
      "source": [
        "### 学習データの準備\n",
        "サンプルのコーパスとしては以降もこれまでと同様に\"You say goodbye and I say hello.\"の１文をコーパスとして利用する。\n",
        "#### コンテキストとターゲット\n",
        "コーパスからコンテキストとターゲットを作成する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4-vml6MPp7q",
        "colab_type": "code",
        "colab": {},
        "outputId": "afb34dcc-a992-4a27-f12b-aaf8bcfbee9c"
      },
      "source": [
        "text = \"You say goodbye and I say hello.\"\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "print(corpus)\n",
        "print(id_to_word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 1 5 6]\n",
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X7WhiEBPp7t",
        "colab_type": "code",
        "colab": {},
        "outputId": "d6635286-2125-4a11-ab81-55f0c63fe719"
      },
      "source": [
        "#create contexts and target\n",
        "#not considering the edge words here\n",
        "def create_contexts_target(corpus, window_size=1):\n",
        "    contexts = []\n",
        "    target = corpus[window_size : -window_size]\n",
        "    \n",
        "    for i in range(window_size, len(corpus) - window_size):\n",
        "        cs = []\n",
        "        for s in range(-window_size, window_size + 1):\n",
        "            if s == 0:\n",
        "                continue\n",
        "            cs.append(corpus[i + s])\n",
        "        contexts.append(cs)\n",
        "        \n",
        "    return np.array(contexts), np.array(target)\n",
        "\n",
        "c, t = create_contexts_target(corpus)\n",
        "print(c.shape)\n",
        "print(t.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 2)\n",
            "(6,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5a4nZvDPp7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#converting contexts and target to one-hot vector representation\n",
        "def one_hot_encoding(target, vocab_size):\n",
        "    I = np.identity(vocab_size)\n",
        "    \n",
        "    return I[target]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZd0k49PPp7z",
        "colab_type": "code",
        "colab": {},
        "outputId": "b9fede08-fc56-430d-99b8-d4f65c63201d"
      },
      "source": [
        "vocab_size = len(word_to_id)\n",
        "t_ohe = one_hot_encoding(t, vocab_size)\n",
        "c_ohe = one_hot_encoding(c, vocab_size)\n",
        "print(c_ohe[:,0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNvyT4LYPp71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from common.layers import MatMul, SoftmaxWithLoss\n",
        "\n",
        "class SimpleCBOW:\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        V, H = vocab_size, hidden_size\n",
        "        \n",
        "        #重みの初期化\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01*np.random.randn(H, V).astype('f')\n",
        "        \n",
        "        #layers\n",
        "        self.in_layer0 = MatMul(W_in)\n",
        "        self.in_layer1 = MatMul(W_in)\n",
        "        self.out_layer = MatMul(W_out)\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "        \n",
        "        #全ての重みと勾配をリストにまとめる\n",
        "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "            \n",
        "        #メンバ変数に単語の分散表現を設定\n",
        "        self.word_vecs = W_in\n",
        "    \n",
        "    #feed forward    \n",
        "    def forward(self, contexts, target):\n",
        "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
        "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
        "        h = (h0 + h1) * 0.5\n",
        "        score = self.out_layer.forward(h)\n",
        "        loss = self.loss_layer.forward(score, target)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    #backprop\n",
        "    def backward(self, dout=1):\n",
        "        ds = self.loss_layer.backward(dout)\n",
        "        da = self.out_layer.backward(ds)\n",
        "        da *= 0.5\n",
        "        self.in_layer1.backward(da)\n",
        "        self.in_layer0.backward(da)\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv6JlW2rPp73",
        "colab_type": "code",
        "colab": {},
        "outputId": "d9270095-f685-4883-9df8-0db439b03b97"
      },
      "source": [
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "\n",
        "window_size = 1\n",
        "hidden_size = 5\n",
        "batch_size = 3\n",
        "max_epoch = 1000\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "contexts, target = create_contexts_target(corpus, window_size=window_size)\n",
        "target = one_hot_encoding(target, vocab_size)\n",
        "contexts = one_hot_encoding(contexts, vocab_size)\n",
        "\n",
        "model = SimpleCBOW(vocab_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch 1 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 2 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 3 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 4 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 5 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 6 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 7 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 8 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 9 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 10 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 11 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 12 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 13 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 14 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 15 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 16 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 17 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 18 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 19 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 20 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 21 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 22 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 23 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 24 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 25 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 26 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 27 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 28 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 29 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 30 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 31 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 32 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 33 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 34 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 35 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 36 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 37 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 38 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 39 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 40 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 41 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 42 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 43 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 44 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 45 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 46 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 47 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 48 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 49 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 50 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 51 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 52 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
            "| epoch 53 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 54 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
            "| epoch 55 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
            "| epoch 56 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
            "| epoch 57 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
            "| epoch 58 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
            "| epoch 59 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
            "| epoch 60 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
            "| epoch 61 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
            "| epoch 62 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
            "| epoch 63 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
            "| epoch 64 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
            "| epoch 65 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
            "| epoch 66 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
            "| epoch 67 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
            "| epoch 68 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
            "| epoch 69 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
            "| epoch 70 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
            "| epoch 71 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
            "| epoch 72 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
            "| epoch 73 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
            "| epoch 74 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
            "| epoch 75 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
            "| epoch 76 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
            "| epoch 77 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
            "| epoch 78 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
            "| epoch 79 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
            "| epoch 80 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
            "| epoch 81 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
            "| epoch 82 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
            "| epoch 83 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
            "| epoch 84 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
            "| epoch 85 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
            "| epoch 86 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
            "| epoch 87 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
            "| epoch 88 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
            "| epoch 89 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
            "| epoch 90 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
            "| epoch 91 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
            "| epoch 92 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
            "| epoch 93 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
            "| epoch 94 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
            "| epoch 95 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
            "| epoch 96 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
            "| epoch 97 |  iter 1 / 2 | time 0[s] | loss 1.78\n",
            "| epoch 98 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
            "| epoch 99 |  iter 1 / 2 | time 0[s] | loss 1.75\n",
            "| epoch 100 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
            "| epoch 101 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
            "| epoch 102 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
            "| epoch 103 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
            "| epoch 104 |  iter 1 / 2 | time 0[s] | loss 1.73\n",
            "| epoch 105 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
            "| epoch 106 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
            "| epoch 107 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
            "| epoch 108 |  iter 1 / 2 | time 0[s] | loss 1.71\n",
            "| epoch 109 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
            "| epoch 110 |  iter 1 / 2 | time 0[s] | loss 1.70\n",
            "| epoch 111 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
            "| epoch 112 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
            "| epoch 113 |  iter 1 / 2 | time 0[s] | loss 1.71\n",
            "| epoch 114 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
            "| epoch 115 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
            "| epoch 116 |  iter 1 / 2 | time 0[s] | loss 1.73\n",
            "| epoch 117 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
            "| epoch 118 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
            "| epoch 119 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
            "| epoch 120 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
            "| epoch 121 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
            "| epoch 122 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
            "| epoch 123 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
            "| epoch 124 |  iter 1 / 2 | time 0[s] | loss 1.63\n",
            "| epoch 125 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
            "| epoch 126 |  iter 1 / 2 | time 0[s] | loss 1.63\n",
            "| epoch 127 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
            "| epoch 128 |  iter 1 / 2 | time 0[s] | loss 1.61\n",
            "| epoch 129 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
            "| epoch 130 |  iter 1 / 2 | time 0[s] | loss 1.62\n",
            "| epoch 131 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
            "| epoch 132 |  iter 1 / 2 | time 0[s] | loss 1.62\n",
            "| epoch 133 |  iter 1 / 2 | time 0[s] | loss 1.63\n",
            "| epoch 134 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
            "| epoch 135 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
            "| epoch 136 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
            "| epoch 137 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
            "| epoch 138 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
            "| epoch 139 |  iter 1 / 2 | time 0[s] | loss 1.62\n",
            "| epoch 140 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
            "| epoch 141 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
            "| epoch 142 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
            "| epoch 143 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
            "| epoch 144 |  iter 1 / 2 | time 0[s] | loss 1.55\n",
            "| epoch 145 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
            "| epoch 146 |  iter 1 / 2 | time 0[s] | loss 1.50\n",
            "| epoch 147 |  iter 1 / 2 | time 0[s] | loss 1.53\n",
            "| epoch 148 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
            "| epoch 149 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
            "| epoch 150 |  iter 1 / 2 | time 0[s] | loss 1.55\n",
            "| epoch 151 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
            "| epoch 152 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
            "| epoch 153 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
            "| epoch 154 |  iter 1 / 2 | time 0[s] | loss 1.53\n",
            "| epoch 155 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
            "| epoch 156 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
            "| epoch 157 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
            "| epoch 158 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
            "| epoch 159 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
            "| epoch 160 |  iter 1 / 2 | time 0[s] | loss 1.54\n",
            "| epoch 161 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
            "| epoch 162 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
            "| epoch 163 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
            "| epoch 164 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
            "| epoch 165 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
            "| epoch 166 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
            "| epoch 167 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
            "| epoch 168 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
            "| epoch 169 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
            "| epoch 170 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
            "| epoch 171 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
            "| epoch 172 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
            "| epoch 173 |  iter 1 / 2 | time 0[s] | loss 1.40\n",
            "| epoch 174 |  iter 1 / 2 | time 0[s] | loss 1.40\n",
            "| epoch 175 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
            "| epoch 176 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
            "| epoch 177 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
            "| epoch 178 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 179 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
            "| epoch 180 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
            "| epoch 181 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
            "| epoch 182 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 183 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
            "| epoch 184 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 185 |  iter 1 / 2 | time 0[s] | loss 1.46\n",
            "| epoch 186 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 187 |  iter 1 / 2 | time 0[s] | loss 1.35\n",
            "| epoch 188 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
            "| epoch 189 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
            "| epoch 190 |  iter 1 / 2 | time 0[s] | loss 1.38\n",
            "| epoch 191 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
            "| epoch 192 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
            "| epoch 193 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
            "| epoch 194 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
            "| epoch 195 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 196 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
            "| epoch 197 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 198 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 199 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
            "| epoch 200 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 201 |  iter 1 / 2 | time 0[s] | loss 1.25\n",
            "| epoch 202 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
            "| epoch 203 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 204 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
            "| epoch 205 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
            "| epoch 206 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 207 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 208 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
            "| epoch 209 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 210 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
            "| epoch 211 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 212 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 213 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 214 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
            "| epoch 215 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
            "| epoch 216 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 217 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 218 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 219 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 220 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 221 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
            "| epoch 222 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 223 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 224 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 225 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 226 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 227 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 228 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 229 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 230 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 231 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 232 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 233 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 234 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 235 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 236 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 237 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 238 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 239 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 240 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 241 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 242 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
            "| epoch 243 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 244 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 245 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 246 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 247 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 248 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 249 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 250 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 251 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 252 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 253 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 254 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 255 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 256 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 257 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 258 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 259 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 260 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 261 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 262 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 263 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 264 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 265 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 266 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 267 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 268 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 269 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 270 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 271 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 272 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 273 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 274 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 275 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 276 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 277 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 278 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 279 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 280 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 281 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 282 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 283 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 284 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 285 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 286 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 287 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 288 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 289 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 290 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 291 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 292 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 293 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 294 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 295 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 296 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 297 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 298 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 299 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 300 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 301 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 302 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 303 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 304 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 305 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 306 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 307 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 308 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 309 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 310 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 311 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 312 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 313 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 314 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 315 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 316 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 317 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 318 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 319 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 320 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
            "| epoch 321 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 322 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 323 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 324 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 325 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 326 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 327 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 328 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 329 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 330 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
            "| epoch 331 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 332 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 333 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 334 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 335 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 336 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 337 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 338 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 339 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 340 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 341 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 342 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 343 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 344 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
            "| epoch 345 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 346 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 347 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 348 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 349 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 350 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 351 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 352 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 353 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
            "| epoch 354 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 355 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 356 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 357 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 358 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
            "| epoch 359 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 360 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 361 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
            "| epoch 362 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 363 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 364 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 365 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 366 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
            "| epoch 367 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 368 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 369 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 370 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 371 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 372 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 373 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 374 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 375 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
            "| epoch 376 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 377 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
            "| epoch 378 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 379 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 380 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 381 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 382 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 383 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
            "| epoch 384 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
            "| epoch 385 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
            "| epoch 386 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
            "| epoch 387 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
            "| epoch 388 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
            "| epoch 389 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
            "| epoch 390 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 391 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
            "| epoch 392 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
            "| epoch 393 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 394 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
            "| epoch 395 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
            "| epoch 396 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
            "| epoch 397 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
            "| epoch 398 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 399 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 400 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
            "| epoch 401 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
            "| epoch 402 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 403 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
            "| epoch 404 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 405 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
            "| epoch 406 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
            "| epoch 407 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 408 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 409 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
            "| epoch 410 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
            "| epoch 411 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 412 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 413 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
            "| epoch 414 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 415 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
            "| epoch 416 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 417 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
            "| epoch 418 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
            "| epoch 419 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 420 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 421 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
            "| epoch 422 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 423 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
            "| epoch 424 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
            "| epoch 425 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 426 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
            "| epoch 427 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
            "| epoch 428 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 429 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
            "| epoch 430 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
            "| epoch 431 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 432 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
            "| epoch 433 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
            "| epoch 434 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
            "| epoch 435 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
            "| epoch 436 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
            "| epoch 437 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
            "| epoch 438 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 439 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
            "| epoch 440 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
            "| epoch 441 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
            "| epoch 442 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
            "| epoch 443 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
            "| epoch 444 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
            "| epoch 445 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
            "| epoch 446 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
            "| epoch 447 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
            "| epoch 448 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
            "| epoch 449 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
            "| epoch 450 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
            "| epoch 451 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
            "| epoch 452 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
            "| epoch 453 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
            "| epoch 454 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
            "| epoch 455 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
            "| epoch 456 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 457 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
            "| epoch 458 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
            "| epoch 459 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 460 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
            "| epoch 461 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
            "| epoch 462 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
            "| epoch 463 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
            "| epoch 464 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
            "| epoch 465 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 466 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
            "| epoch 467 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 468 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 469 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
            "| epoch 470 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
            "| epoch 471 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
            "| epoch 472 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 473 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 474 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
            "| epoch 475 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
            "| epoch 476 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 477 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 478 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
            "| epoch 479 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 480 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
            "| epoch 481 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 482 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
            "| epoch 483 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
            "| epoch 484 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
            "| epoch 485 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 486 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 487 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
            "| epoch 488 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
            "| epoch 489 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
            "| epoch 490 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 491 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 492 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
            "| epoch 493 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
            "| epoch 494 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
            "| epoch 495 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
            "| epoch 496 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 497 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
            "| epoch 498 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
            "| epoch 499 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
            "| epoch 500 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
            "| epoch 501 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
            "| epoch 502 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
            "| epoch 503 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 504 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 505 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 506 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 507 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 508 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
            "| epoch 509 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
            "| epoch 510 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
            "| epoch 511 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 512 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
            "| epoch 513 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
            "| epoch 514 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
            "| epoch 515 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
            "| epoch 516 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
            "| epoch 517 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
            "| epoch 518 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
            "| epoch 519 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
            "| epoch 520 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 521 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
            "| epoch 522 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
            "| epoch 523 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 524 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
            "| epoch 525 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 526 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 527 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 528 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
            "| epoch 529 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
            "| epoch 530 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 531 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
            "| epoch 532 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 533 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
            "| epoch 534 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 535 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 536 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
            "| epoch 537 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 538 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 539 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
            "| epoch 540 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
            "| epoch 541 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 542 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 543 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
            "| epoch 544 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 545 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
            "| epoch 546 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
            "| epoch 547 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
            "| epoch 548 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 549 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
            "| epoch 550 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 551 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
            "| epoch 552 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
            "| epoch 553 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 554 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
            "| epoch 555 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 556 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 557 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 558 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
            "| epoch 559 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 560 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
            "| epoch 561 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
            "| epoch 562 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
            "| epoch 563 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
            "| epoch 564 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
            "| epoch 565 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
            "| epoch 566 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 567 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
            "| epoch 568 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 569 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
            "| epoch 570 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 571 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 572 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
            "| epoch 573 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
            "| epoch 574 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 575 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
            "| epoch 576 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 577 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
            "| epoch 578 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
            "| epoch 579 |  iter 1 / 2 | time 0[s] | loss 0.28\n",
            "| epoch 580 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
            "| epoch 581 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 582 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 583 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 584 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
            "| epoch 585 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
            "| epoch 586 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
            "| epoch 587 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
            "| epoch 588 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 589 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 590 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
            "| epoch 591 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 592 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
            "| epoch 593 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
            "| epoch 594 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
            "| epoch 595 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
            "| epoch 596 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
            "| epoch 597 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 598 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
            "| epoch 599 |  iter 1 / 2 | time 0[s] | loss 0.26\n",
            "| epoch 600 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
            "| epoch 601 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
            "| epoch 602 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 603 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 604 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
            "| epoch 605 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
            "| epoch 606 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 607 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
            "| epoch 608 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 609 |  iter 1 / 2 | time 0[s] | loss 0.35\n",
            "| epoch 610 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 611 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
            "| epoch 612 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 613 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
            "| epoch 614 |  iter 1 / 2 | time 0[s] | loss 0.35\n",
            "| epoch 615 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 616 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 617 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
            "| epoch 618 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 619 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
            "| epoch 620 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
            "| epoch 621 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 622 |  iter 1 / 2 | time 0[s] | loss 0.29\n",
            "| epoch 623 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
            "| epoch 624 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 625 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
            "| epoch 626 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 627 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 628 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
            "| epoch 629 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 630 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 631 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
            "| epoch 632 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 633 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
            "| epoch 634 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 635 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 636 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
            "| epoch 637 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 638 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 639 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
            "| epoch 640 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 641 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 642 |  iter 1 / 2 | time 0[s] | loss 0.23\n",
            "| epoch 643 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
            "| epoch 644 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 645 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 646 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 647 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 648 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 649 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
            "| epoch 650 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 651 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 652 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 653 |  iter 1 / 2 | time 0[s] | loss 0.28\n",
            "| epoch 654 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 655 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 656 |  iter 1 / 2 | time 0[s] | loss 0.35\n",
            "| epoch 657 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 658 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 659 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 660 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 661 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 662 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
            "| epoch 663 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 664 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 665 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 666 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 667 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 668 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 669 |  iter 1 / 2 | time 0[s] | loss 0.28\n",
            "| epoch 670 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
            "| epoch 671 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 672 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
            "| epoch 673 |  iter 1 / 2 | time 0[s] | loss 0.27\n",
            "| epoch 674 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
            "| epoch 675 |  iter 1 / 2 | time 0[s] | loss 0.24\n",
            "| epoch 676 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
            "| epoch 677 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 678 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 679 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 680 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 681 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 682 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 683 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
            "| epoch 684 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 685 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 686 |  iter 1 / 2 | time 0[s] | loss 0.26\n",
            "| epoch 687 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 688 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
            "| epoch 689 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
            "| epoch 690 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 691 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 692 |  iter 1 / 2 | time 0[s] | loss 0.22\n",
            "| epoch 693 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 694 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
            "| epoch 695 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 696 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 697 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
            "| epoch 698 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 699 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 700 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 701 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 702 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
            "| epoch 703 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
            "| epoch 704 |  iter 1 / 2 | time 0[s] | loss 0.29\n",
            "| epoch 705 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
            "| epoch 706 |  iter 1 / 2 | time 0[s] | loss 0.28\n",
            "| epoch 707 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
            "| epoch 708 |  iter 1 / 2 | time 0[s] | loss 0.19\n",
            "| epoch 709 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 710 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 711 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
            "| epoch 712 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 713 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
            "| epoch 714 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
            "| epoch 715 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 716 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
            "| epoch 717 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 718 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 719 |  iter 1 / 2 | time 0[s] | loss 0.25\n",
            "| epoch 720 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
            "| epoch 721 |  iter 1 / 2 | time 0[s] | loss 0.27\n",
            "| epoch 722 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
            "| epoch 723 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 724 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
            "| epoch 725 |  iter 1 / 2 | time 0[s] | loss 0.28\n",
            "| epoch 726 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 727 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 728 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
            "| epoch 729 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 730 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 731 |  iter 1 / 2 | time 0[s] | loss 0.24\n",
            "| epoch 732 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 733 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 734 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 735 |  iter 1 / 2 | time 0[s] | loss 0.27\n",
            "| epoch 736 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 737 |  iter 1 / 2 | time 0[s] | loss 0.27\n",
            "| epoch 738 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
            "| epoch 739 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 740 |  iter 1 / 2 | time 0[s] | loss 0.26\n",
            "| epoch 741 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
            "| epoch 742 |  iter 1 / 2 | time 0[s] | loss 0.28\n",
            "| epoch 743 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
            "| epoch 744 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 745 |  iter 1 / 2 | time 0[s] | loss 0.26\n",
            "| epoch 746 |  iter 1 / 2 | time 0[s] | loss 0.27\n",
            "| epoch 747 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
            "| epoch 748 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 749 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 750 |  iter 1 / 2 | time 0[s] | loss 0.27\n",
            "| epoch 751 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
            "| epoch 752 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 753 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 754 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
            "| epoch 755 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 756 |  iter 1 / 2 | time 0[s] | loss 0.26\n",
            "| epoch 757 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
            "| epoch 758 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 759 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 760 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 761 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 762 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 763 |  iter 1 / 2 | time 0[s] | loss 0.26\n",
            "| epoch 764 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
            "| epoch 765 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 766 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 767 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
            "| epoch 768 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 769 |  iter 1 / 2 | time 0[s] | loss 0.15\n",
            "| epoch 770 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
            "| epoch 771 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 772 |  iter 1 / 2 | time 0[s] | loss 0.27\n",
            "| epoch 773 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 774 |  iter 1 / 2 | time 0[s] | loss 0.28\n",
            "| epoch 775 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 776 |  iter 1 / 2 | time 0[s] | loss 0.26\n",
            "| epoch 777 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
            "| epoch 778 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 779 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
            "| epoch 780 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 781 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 782 |  iter 1 / 2 | time 0[s] | loss 0.16\n",
            "| epoch 783 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
            "| epoch 784 |  iter 1 / 2 | time 0[s] | loss 0.27\n",
            "| epoch 785 |  iter 1 / 2 | time 0[s] | loss 0.35\n",
            "| epoch 786 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 787 |  iter 1 / 2 | time 0[s] | loss 0.35\n",
            "| epoch 788 |  iter 1 / 2 | time 0[s] | loss 0.35\n",
            "| epoch 789 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 790 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 791 |  iter 1 / 2 | time 0[s] | loss 0.26\n",
            "| epoch 792 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 793 |  iter 1 / 2 | time 0[s] | loss 0.35\n",
            "| epoch 794 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 795 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 796 |  iter 1 / 2 | time 0[s] | loss 0.25\n",
            "| epoch 797 |  iter 1 / 2 | time 0[s] | loss 0.25\n",
            "| epoch 798 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
            "| epoch 799 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 800 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 801 |  iter 1 / 2 | time 0[s] | loss 0.35\n",
            "| epoch 802 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 803 |  iter 1 / 2 | time 0[s] | loss 0.27\n",
            "| epoch 804 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 805 |  iter 1 / 2 | time 0[s] | loss 0.24\n",
            "| epoch 806 |  iter 1 / 2 | time 0[s] | loss 0.35\n",
            "| epoch 807 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 808 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 809 |  iter 1 / 2 | time 0[s] | loss 0.27\n",
            "| epoch 810 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 811 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 812 |  iter 1 / 2 | time 0[s] | loss 0.22\n",
            "| epoch 813 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
            "| epoch 814 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 815 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 816 |  iter 1 / 2 | time 0[s] | loss 0.26\n",
            "| epoch 817 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 818 |  iter 1 / 2 | time 0[s] | loss 0.26\n",
            "| epoch 819 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 820 |  iter 1 / 2 | time 0[s] | loss 0.26\n",
            "| epoch 821 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 822 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 823 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 824 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 825 |  iter 1 / 2 | time 0[s] | loss 0.11\n",
            "| epoch 826 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
            "| epoch 827 |  iter 1 / 2 | time 0[s] | loss 0.24\n",
            "| epoch 828 |  iter 1 / 2 | time 0[s] | loss 0.23\n",
            "| epoch 829 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 830 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 831 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 832 |  iter 1 / 2 | time 0[s] | loss 0.23\n",
            "| epoch 833 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
            "| epoch 834 |  iter 1 / 2 | time 0[s] | loss 0.15\n",
            "| epoch 835 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 836 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 837 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 838 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 839 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
            "| epoch 840 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 841 |  iter 1 / 2 | time 0[s] | loss 0.23\n",
            "| epoch 842 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
            "| epoch 843 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 844 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 845 |  iter 1 / 2 | time 0[s] | loss 0.24\n",
            "| epoch 846 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 847 |  iter 1 / 2 | time 0[s] | loss 0.35\n",
            "| epoch 848 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 849 |  iter 1 / 2 | time 0[s] | loss 0.22\n",
            "| epoch 850 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 851 |  iter 1 / 2 | time 0[s] | loss 0.35\n",
            "| epoch 852 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 853 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 854 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 855 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 856 |  iter 1 / 2 | time 0[s] | loss 0.24\n",
            "| epoch 857 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 858 |  iter 1 / 2 | time 0[s] | loss 0.14\n",
            "| epoch 859 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 860 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 861 |  iter 1 / 2 | time 0[s] | loss 0.23\n",
            "| epoch 862 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 863 |  iter 1 / 2 | time 0[s] | loss 0.22\n",
            "| epoch 864 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 865 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
            "| epoch 866 |  iter 1 / 2 | time 0[s] | loss 0.11\n",
            "| epoch 867 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 868 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 869 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 870 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 871 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 872 |  iter 1 / 2 | time 0[s] | loss 0.24\n",
            "| epoch 873 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 874 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 875 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 876 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 877 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 878 |  iter 1 / 2 | time 0[s] | loss 0.24\n",
            "| epoch 879 |  iter 1 / 2 | time 0[s] | loss 0.21\n",
            "| epoch 880 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 881 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
            "| epoch 882 |  iter 1 / 2 | time 0[s] | loss 0.23\n",
            "| epoch 883 |  iter 1 / 2 | time 0[s] | loss 0.20\n",
            "| epoch 884 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
            "| epoch 885 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 886 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 887 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 888 |  iter 1 / 2 | time 0[s] | loss 0.20\n",
            "| epoch 889 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
            "| epoch 890 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 891 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 892 |  iter 1 / 2 | time 0[s] | loss 0.23\n",
            "| epoch 893 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 894 |  iter 1 / 2 | time 0[s] | loss 0.21\n",
            "| epoch 895 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 896 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
            "| epoch 897 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 898 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 899 |  iter 1 / 2 | time 0[s] | loss 0.22\n",
            "| epoch 900 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 901 |  iter 1 / 2 | time 0[s] | loss 0.12\n",
            "| epoch 902 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 903 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 904 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
            "| epoch 905 |  iter 1 / 2 | time 0[s] | loss 0.09\n",
            "| epoch 906 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 907 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 908 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 909 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 910 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 911 |  iter 1 / 2 | time 0[s] | loss 0.10\n",
            "| epoch 912 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 913 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 914 |  iter 1 / 2 | time 0[s] | loss 0.22\n",
            "| epoch 915 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 916 |  iter 1 / 2 | time 0[s] | loss 0.20\n",
            "| epoch 917 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 918 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 919 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 920 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 921 |  iter 1 / 2 | time 0[s] | loss 0.19\n",
            "| epoch 922 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 923 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 924 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 925 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 926 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 927 |  iter 1 / 2 | time 0[s] | loss 0.20\n",
            "| epoch 928 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 929 |  iter 1 / 2 | time 0[s] | loss 0.20\n",
            "| epoch 930 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 931 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 932 |  iter 1 / 2 | time 0[s] | loss 0.20\n",
            "| epoch 933 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 934 |  iter 1 / 2 | time 0[s] | loss 0.20\n",
            "| epoch 935 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 936 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 937 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 938 |  iter 1 / 2 | time 0[s] | loss 0.19\n",
            "| epoch 939 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 940 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
            "| epoch 941 |  iter 1 / 2 | time 0[s] | loss 0.19\n",
            "| epoch 942 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 943 |  iter 1 / 2 | time 0[s] | loss 0.33\n",
            "| epoch 944 |  iter 1 / 2 | time 0[s] | loss 0.19\n",
            "| epoch 945 |  iter 1 / 2 | time 0[s] | loss 0.21\n",
            "| epoch 946 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 947 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 948 |  iter 1 / 2 | time 0[s] | loss 0.18\n",
            "| epoch 949 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 950 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 951 |  iter 1 / 2 | time 0[s] | loss 0.21\n",
            "| epoch 952 |  iter 1 / 2 | time 0[s] | loss 0.29\n",
            "| epoch 953 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 954 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 955 |  iter 1 / 2 | time 0[s] | loss 0.21\n",
            "| epoch 956 |  iter 1 / 2 | time 0[s] | loss 0.29\n",
            "| epoch 957 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 958 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 959 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 960 |  iter 1 / 2 | time 0[s] | loss 0.19\n",
            "| epoch 961 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 962 |  iter 1 / 2 | time 0[s] | loss 0.19\n",
            "| epoch 963 |  iter 1 / 2 | time 0[s] | loss 0.21\n",
            "| epoch 964 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
            "| epoch 965 |  iter 1 / 2 | time 0[s] | loss 0.20\n",
            "| epoch 966 |  iter 1 / 2 | time 0[s] | loss 0.29\n",
            "| epoch 967 |  iter 1 / 2 | time 0[s] | loss 0.21\n",
            "| epoch 968 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 969 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 970 |  iter 1 / 2 | time 0[s] | loss 0.21\n",
            "| epoch 971 |  iter 1 / 2 | time 0[s] | loss 0.29\n",
            "| epoch 972 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 973 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 974 |  iter 1 / 2 | time 0[s] | loss 0.19\n",
            "| epoch 975 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
            "| epoch 976 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 977 |  iter 1 / 2 | time 0[s] | loss 0.29\n",
            "| epoch 978 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
            "| epoch 979 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 980 |  iter 1 / 2 | time 0[s] | loss 0.29\n",
            "| epoch 981 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 982 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 983 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 984 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 985 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 986 |  iter 1 / 2 | time 0[s] | loss 0.19\n",
            "| epoch 987 |  iter 1 / 2 | time 0[s] | loss 0.19\n",
            "| epoch 988 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 989 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 990 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 991 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
            "| epoch 992 |  iter 1 / 2 | time 0[s] | loss 0.19\n",
            "| epoch 993 |  iter 1 / 2 | time 0[s] | loss 0.19\n",
            "| epoch 994 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
            "| epoch 995 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 996 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
            "| epoch 997 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 998 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 999 |  iter 1 / 2 | time 0[s] | loss 0.30\n",
            "| epoch 1000 |  iter 1 / 2 | time 0[s] | loss 0.28\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OipWaDppPp78",
        "colab_type": "text"
      },
      "source": [
        "### word2vecに関する補足\n",
        "CBOWモデルはコンテキストが与えられた上で、ターゲットとなる単語の確率を出力する。<br>\n",
        "今ウィンドウサイズ1のモデルを考えた時、コンテキスト$w_{t-1},w_{t+1}$に対してターゲットtが得られる確率は事後確率の表記で以下のように表せる。\n",
        "\n",
        "$$P(w_{t}|w_{t-1}, w_{t+1})$$\n",
        "\n",
        "CBOWはまさにこの事後確率をモデル化していると言える。<br>\n",
        "交差エントロピー誤差$$L = -\\sum_k t_k \\log{y_k}$$を当てはめてやる。\n",
        "(t_kは教師ラベルでone-hotベクトル)\n",
        "\n",
        "するとコンテキストが$w_{t-1}$、$w_{t+1}$の時のCBOWモデルの出力に対する誤差は、\n",
        "$$L = - \\log{P(w_t | w_{t-1}, w_{t+1})}$$\n",
        "と書ける。\n",
        "\n",
        "コーパス全体に拡張すると、損失関数として以下のように表現できる。\n",
        "$$L = -\\frac{1}{T}\\sum_{t=1}^{T} \\log{P(w_t|w_{t-1}, w_{t+1})}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PLcothNPp79",
        "colab_type": "text"
      },
      "source": [
        "## skip-gramモデル\n",
        "word2vecで提案されているもう一つのモデル。<br>\n",
        "CBOWとは逆に中央の単語（ターゲット）から周囲の単語（コンテキスト）を推定するモデル。\n",
        "\n",
        "確率表記で示すと、「$w_t$が与えられた時の$w_{t-1}, w_{t+1}$が**同時に起こる**確率」を求めることになり、\n",
        "$$P(w_{t-1}, w_{t+1}|w_t)$$\n",
        "となる。<br>\n",
        "\n",
        "なお、ここではコンテキストの単語間には関連性がないと仮定する（＝条件付き独立であると仮定する）。<br>\n",
        "したがって、\n",
        "$$P(w_{t-1}, w_{t+1}|w_t) = P(w_{t-1}|w_t)P(w_{t+1}|w_t)$$\n",
        "と分解できる。\n",
        "\n",
        "交差エントロピー誤差に適用すると、\n",
        "$$\n",
        "\\begin{align}\n",
        "L &= -\\log{P(w_{t-1}, w_{t+1}|w_t)}\\\\&= -\\log{P(w_{t-1}|w_t)P(w_{t+1}|w_t)}\\\\&= -(\\log{P(w_{t-1}|w_t)} + \\log{P(w_{t+1}|w_t)})\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "コーパス全体に拡張すると、\n",
        "$$L = -\\frac{1}{T}\\sum_{t=1}^{T} (\\log{P(w_{t-1}|w_t)} + \\log{P(w_{t+1}|w_t))}$$\n",
        "\n",
        "\n",
        "skip-gramはコンテキストの数だけ推測を行うため、損失関数は各コンテキストで求めた損失の総和となる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSgj1yp4Pp7-",
        "colab_type": "text"
      },
      "source": [
        "### CBOW vs skip-gram\n",
        "* 精度の点においてはskip-gramの方が優秀\n",
        "* 学習速度はCBOWの方が速い"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0LH_I_UPp7-",
        "colab_type": "text"
      },
      "source": [
        "# まとめ\n",
        "## カウントベース vs 推論ベース\n",
        "* カウントベース：コーパス全体の統計データから1回の処理（共起行列(ppmi)→SVD）で単語の分散表現を獲得する手法。\n",
        "* 推論ベース：コーパスを部分的に見ていき、学習しながら単語の分散表現を得ようとする手法。\n",
        "* カウントベースの欠点としては、新たな単語を追加するときには分散表現を1から計算し直して更新する必要がある。\n",
        "* 推論ベースでは学習済みのモデルに新たな単語の情報を追加して再学習させることで分散表現を更新できるため、カウントベースに比べて楽。\n",
        "* 推論ベースの方が単語間の複雑な関係性も捉えることが可能だが、単語間の類似性という点においてはカウントベースと大差ない精度。\n",
        "* ある条件下ではそれぞれの手法は同値であることも分かっている。(skip-gram & negative-sampling)\n",
        "* GloVeというカウントベースと推論ベースをmixした手法も提案されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga7ijqERPp7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}